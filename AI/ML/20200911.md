### Gradient descent algorithm

- Minimize cost function
- Gradeint descent is used many minimization problems
- For a given cost function, `cost(W,b)`, it will find W, b to minimize cost
- It can be applied to more general function cost(w1, w2, ...)



1. Start with initial guesses
   - Start at (0,0) (or any other value)
   - Keeping  changing W and b a little bit to try and reduce cost(W,b)
2. Each time you change the parameters, you select the gradient which reduces cost(W,b) the most possible
3. Repeat
4. Do so until you converge to a local minimum
5. Has an interesting property
   - Where you start can determine which minimum you end up



결국 경사도를 구하기 위한것 - > 미분으로써 경사도를 구함

##### Formal definition

$$
W:= W-\alpha \frac{1}{m}\sum_{i=1}^{m}(Wx^{(i)}-y^{(i)})x^{(i)}
$$

Cost function을 설계 할 때 Convex function이 되는지를 확인해야 함



```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

X = [1,2,3]
Y = [1,2,3]

W = tf.Variable(-3.0)
hypothesis = X * W
cost = tf.reduce_mean(tf.square(hypothesis - Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(100):
    print(step, sess.run(W))
    sess.run(train)
```

1 0.7333336
2 0.98222226
3 0.9988148
4 0.99992096
5 0.9999947
6 0.99999964
7 0.99999994
8 1.0
9 1.0
10 1.0

![](https://github.com/bongwon-suh/TIL/blob/master/img/0911_5.JPG?raw=true)

