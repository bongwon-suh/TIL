# Logistic (regression) classification

### Binary Classification

- 둘중의 한개의 값을 고름
  - Spam Detection : Spam(1) or Ham(0)
  - Facebook feed : show(1) or hide(0)



### Logistic Hypothesis

- $$
  H(X)=\frac{1}{a+e^{-W^TX}}
  $$



### cost Function

- 기존 cost function 사용시 시작 위치에 따라 minimum 값이 달라 질 수 있으므로 사용 불가

- $$
  cost(W)=\frac{1}{m}\sum c(H(x),y)
  $$

- $$
  c(H(x),y)=
  \left\{\begin{matrix}-log(H(x)) : y=1
  \\ 
  -log(1-H(x)) : y=0
  \end{matrix}\right.
  $$

- $$
  C(H(x),y)=ylog(H(x))-(1-y)log(1-H(x))
  $$



### Minimize cost

- $$
  W :=W-\alpha \frac{\partial }{\partial W}cost(W)
  $$

- 



``` python
import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
tf.set_random_seed(777)

xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)
x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]

# print(x_data.shape, x_data, len(x_data))
# print(y_data.shape, y_data)

X = tf.placeholder(tf.float32, shape=[None, 3])
Y = tf.placeholder(tf.float32, shape=[None, 1])

W = tf.Variable(tf.random_normal([3,1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = tf.matmul(X,W) + b
cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
    if step % 50 == 0:
        print(step, "Cost : ", cost_val, "\nPrediction : \n", hy_val)

print("Score will be : ", sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))
print("Scores will be : ", sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))

```