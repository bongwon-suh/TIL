# Learning Rate

Gradient descent 알고리즘에 의해 다음 스탭을 찾아가기 위한것



### overshooting

![](https://github.com/bongwon-suh/TIL/blob/master/img/0917/03.JPG?raw=true)

그림과 같이 learning rate 값이 클 경우 기울기의 최솟값을 구하는게 아니라 반대로 다시 올라 갈 수도 있는데 이를 overshooting이라고 부름



### Samll learning rate

![](https://github.com/bongwon-suh/TIL/blob/master/img/0917/04.JPG?raw=true)

Overshooting 과 반대로 learning rate 값이 매우 작을 경우 최솟값을 구하기에 훨씬 많은 학습이 필요함



### Try sevral learning rate

- Observe the cost function
- Check it goes down in a reasonable rate 





